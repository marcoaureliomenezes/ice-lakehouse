{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef256f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9281d721-e21e-4175-a641-e98d5cb6c909;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 519ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9281d721-e21e-4175-a641-e98d5cb6c909\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/13ms)\n",
      "24/09/28 03:51:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Instantiated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58266)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/python/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/bitnami/python/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/bitnami/python/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/bitnami/python/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/bitnami/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/bitnami/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "NESSIE_URI = \"http://nessie:19120/api/v1\"\n",
    "MINIO_HOST = 'http://minio:9000'\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\")\n",
    "\n",
    "\n",
    "jar_packages = [\n",
    "  \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1\",\n",
    "  \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.95.0\",\n",
    "  \"software.amazon.awssdk:bundle:2.17.178\",\n",
    "  \"software.amazon.awssdk:url-connection-client:2.17.178\"\n",
    "]\n",
    "\n",
    "spark_extensions = [\n",
    "  \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "  \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n",
    "]\n",
    "\n",
    "# fs.s3a.path.style.access true\n",
    "# dremio.s3.compat true\n",
    "# fs.s3a.endpoint http://minio:9000\n",
    "\n",
    "conf = (\n",
    "  pyspark.SparkConf()\n",
    "    .setAppName('iceberg_hello_world')\n",
    "    .set('spark.jars.packages', ','.join(jar_packages))\n",
    "\t.set('spark.sql.extensions', ','.join(spark_extensions))\n",
    "\t.set('spark.sql.catalog.nessie', \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "\t.set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "\t.set('spark.sql.catalog.nessie.ref', 'main')\n",
    "\t.set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "\t.set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "\t.set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "\t.set('spark.sql.catalog.nessie.s3.endpoint', MINIO_HOST)\n",
    "\t.set('spark.sql.catalog.nessie.warehouse', 's3a://warehouse')\n",
    "\t.set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "\t.set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    "\t# .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "    # .set('spark.hadoop.fs.s3a.connection.timeout', '600000')\n",
    "\t# .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\t# .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "\t# .set('spark.hadoop.fs.s3a.endpoint', MINIO_HOST)\t\t\n",
    ")\n",
    "\n",
    "# .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://SEU_MINIO_ENDPOINT:9000\") \\\n",
    "# .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    \n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).master(\"spark://spark-master:7077\").getOrCreate()\n",
    "print(\"Spark Instantiated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ea4098-e410-47b2-aff8-68113fabb643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/28 03:51:52 WARN TaskSetManager: Stage 0 contains a task of very large size (1641 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- campo_int: long (nullable = true)\n",
      " |-- campo_float: double (nullable = true)\n",
      " |-- campo_float_normal: double (nullable = true)\n",
      " |-- campo_booleano: boolean (nullable = true)\n",
      " |-- campo_categorico: string (nullable = true)\n",
      " |-- campo_categorico_faker_injection: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      " |-- campo_categorico_com_peso: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import faker\n",
    "from rand_engine.bulk.benchmarks import Benchmark\n",
    "from rand_engine.bulk.dataframe_builder import BulkRandEngine\n",
    "from rand_engine.bulk.core_distincts import CoreDistincts\n",
    "from rand_engine.bulk.core_numeric import CoreNumeric\n",
    "from rand_engine.bulk.core_datetime import CoreDatetime\n",
    "from rand_engine.bulk.templates import RandEngineTemplates\n",
    "\n",
    "def gen_bulk_data(spark):\n",
    "\n",
    "\n",
    "\n",
    "  bulk_rand_engine = BulkRandEngine()\n",
    "  fake = faker.Faker(locale=\"pt_BR\")\n",
    "\n",
    "\n",
    "  metadata = {\n",
    "  \"campo_int\": dict(method=CoreNumeric.gen_ints, parms=dict(min=0, max=100)),\n",
    "  \"campo_float\": dict(method=CoreNumeric.gen_floats, parms=dict(min=0, max=10**3, round=2)),\n",
    "  \"campo_float_normal\": dict(method=CoreNumeric.gen_floats_normal, parms=dict(mean=10**3, std=10**1, round=2)),\n",
    "  \"campo_booleano\": dict(method=CoreDistincts.gen_distincts_typed, parms=dict(distinct=[True, False])),\n",
    "  \"campo_categorico\": dict(method=CoreDistincts.gen_distincts_typed, parms=dict(distinct=[\"valor_1\", \"valor_2\", \"valor_3\"])),\n",
    "  \"campo_categorico_faker_injection\": dict(method=CoreDistincts.gen_distincts_typed, parms=dict(distinct=[fake.job() for _ in range(100)])),\n",
    "  \"signup_date\": dict(method=CoreDatetime.gen_timestamps, parms=dict(start=\"01-01-2020\", end=\"31-12-2020\", format=\"%d-%m-%Y\")),\n",
    "  \"campo_categorico_com_peso\": dict(\n",
    "    method=CoreDistincts.gen_distincts_typed,\n",
    "    parms=dict(distinct=bulk_rand_engine.handle_distincts_proportions({\"MEI\": 100,\"ME\":23, \"EPP\": 12, \"EMP\": 13, \"EGP\": 1}, 1)))\n",
    "  }\n",
    "\n",
    "  df = bulk_rand_engine.create_spark_df(spark, 10**4, metadata)\n",
    "  return df\n",
    "\n",
    "df_test = gen_bulk_data(spark)\n",
    "df_test.count()\n",
    "df_test.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cebca7-e28b-4113-ba0e-0a791e710140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "24/09/28 03:51:57 WARN TaskSetManager: Stage 3 contains a task of very large size (1641 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS nessie.test_bulk (age LONG, salary DOUBLE, height DOUBLE, is_active BOOLEAN, type STRING, job STRING, signup_date TIMESTAMP, company_size STRING) USING iceberg\")\n",
    "df_test.write.mode(\"overwrite\").saveAsTable(\"nessie.test_bulk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eaf9ef3-65b8-4a27-88b3-b22c0e7ca484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+--------------+----------------+--------------------------------+-------------------+-------------------------+\n",
      "|campo_int|campo_float|campo_float_normal|campo_booleano|campo_categorico|campo_categorico_faker_injection|        signup_date|campo_categorico_com_peso|\n",
      "+---------+-----------+------------------+--------------+----------------+--------------------------------+-------------------+-------------------------+\n",
      "|       48|     256.28|           1026.61|          true|         valor_3|                      Matemático|1974-09-25 14:54:20|                      MEI|\n",
      "|       15|     581.58|           1003.37|          true|         valor_1|            Vistoriador de si...|1989-10-20 02:34:27|                      MEI|\n",
      "|       82|     438.69|            998.86|         false|         valor_3|                         Ecólogo|1996-02-03 13:36:08|                       ME|\n",
      "|       94|      63.97|           1014.65|         false|         valor_2|                    Angiologista|2020-09-22 05:10:25|                      MEI|\n",
      "|       51|     549.79|            985.33|         false|         valor_1|               Diretor de cinema|1991-04-02 10:31:08|                      MEI|\n",
      "|       19|     816.25|             994.7|         false|         valor_1|                    Cinegrafista|1973-05-05 09:14:50|                      MEI|\n",
      "|       94|      45.01|            999.75|          true|         valor_2|                      Comandante|2005-08-24 15:16:30|                       ME|\n",
      "|       33|     513.03|           1013.13|          true|         valor_2|                         Coronel|2002-05-02 15:25:58|                       ME|\n",
      "|       46|       5.72|            989.24|         false|         valor_3|            Instalador de lin...|1995-09-03 11:27:54|                      MEI|\n",
      "|       38|     658.11|           1005.48|          true|         valor_1|                         Ecólogo|1996-05-22 04:37:21|                      MEI|\n",
      "|        9|      239.2|           1011.57|          true|         valor_1|                       Arquiteto|2014-06-28 00:30:24|                      MEI|\n",
      "|       15|     923.66|           1009.02|         false|         valor_3|            Tecnólogo em prod...|1990-12-27 07:44:55|                      MEI|\n",
      "|       57|     824.85|            985.27|          true|         valor_3|                         Ecólogo|2020-05-07 12:54:12|                      MEI|\n",
      "|       71|     926.28|            989.03|         false|         valor_2|                      Pecuarista|1985-02-22 16:53:23|                      MEI|\n",
      "|       84|     693.66|            993.02|          true|         valor_1|                    Cinegrafista|1973-07-01 22:09:22|                      EMP|\n",
      "|       54|     474.16|           1001.66|         false|         valor_3|                      Paramédico|2012-09-24 22:50:11|                      MEI|\n",
      "|        3|      907.1|           1009.23|          true|         valor_3|            Gestor de tecnolo...|2018-07-04 04:29:11|                      MEI|\n",
      "|       73|     211.79|           1020.97|         false|         valor_2|                      Urologista|1997-09-27 15:05:01|                      MEI|\n",
      "|       76|     467.31|            996.89|          true|         valor_2|                         Tenente|1995-02-09 18:34:49|                      EMP|\n",
      "|       29|     135.94|            988.44|         false|         valor_1|                     Programador|1992-06-16 08:21:21|                      EPP|\n",
      "+---------+-----------+------------------+--------------+----------------+--------------------------------+-------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"nessie.test_bulk\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc31445-edb7-4400-95fa-7dfa50e364c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_stage = [\n",
    "#     (3, \"Alecio\", 33),\n",
    "#     (4, \"Rosemberg\", 31),\n",
    "#     (5, \"Felipe\", 35)\n",
    "# ]\n",
    "# df_stage = spark.createDataFrame(data_stage, [\"id\", \"name\", \"age\"])\n",
    "# df_stage.createOrReplaceTempView(\"view_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac956af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"USE nessie\"\"\")\n",
    "\n",
    "# spark.sql(\"CREATE OR REPLACE TABLE test_table (id BIGINT,name STRING, age INT)\")\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# INSERT INTO test_table\n",
    "# VALUES\n",
    "# \t(1, 'Alice', 30),\n",
    "# \t(2, 'Bob', 25),\n",
    "# \t(3, 'Charlie', 35)\n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "# spark.sql(\"\"\"SELECT * FROM test_table\"\"\").show()\n",
    "# spark.sql(\"\"\"SELECT * FROM view_stage\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcb314e-cc75-428d-bc84-8727cb96fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# MERGE INTO test_table\n",
    "# USING (SELECT * FROM view_stage) b\n",
    "# ON test_table.id = b.id\n",
    "#     WHEN MATCHED THEN UPDATE SET test_table.name = b.name\n",
    "#     WHEN NOT MATCHED THEN INSERT *\n",
    "# \"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"SELECT * FROM nessie.test_table\"\"\").distinct().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
