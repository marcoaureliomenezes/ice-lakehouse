{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Spark Driver**: The driver is the program space in which Spark Application's main method runs coodinating the Spark entire application.\n",
    "\n",
    "## Executors\n",
    "\n",
    "- An executor is a processing engine running on each worker node.\n",
    "\n",
    "3. If there are more slots than tasks the spark job will likely not run as efficiently as possible.\n",
    "4. The most granular unit of work in Spark is a task.\n",
    "5. What is correct about Spark Jobs?\n",
    "- Jobs are broken down into stages.\n",
    "- There are multiple tasks within a single job when a DataFrame has more than one partition.\n",
    "- Jobs are collections of tasks that are divided up based on when an action is called.\n",
    "- Jobs are collections of tasks that are divided based on physical plan optimizations.\n",
    "\n",
    "6. Operations to most likely to result in a Shuffle:\n",
    "\n",
    "- Join\n",
    "- GroupBy\n",
    "\n",
    "7. The default value of `spark.sql.shuffle.partitions` is 200. Therefore, by default dataframes will be split into 200 unique partitions when data is being shuffled.\n",
    "8. A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger.\n",
    "\n",
    "9. Operations classified as an action:\n",
    "- take(n): returns an array with the first n elements of the dataset.\n",
    "- collect(): returns all the elements of the dataset as an array at the driver program.\n",
    "- count(): returns the number of elements in the dataset.\n",
    "- show():\n",
    "\n",
    "10. Dataframe operations classified as wide transformations:\n",
    "\n",
    "- groupBy()\n",
    "- join()\n",
    "- sort()\n",
    "\n",
    "11. The cluster execution mode runs the driver on a worker node within a cluster, while the client execution mode runs the driver on the client machine (also known as a gateway or edge node).\n",
    "\n",
    "12. Spark Stability capabilities:\n",
    "\n",
    "- Spark is designed to support the loss of any set of worker nodes, using lineage information to recover lost data.\n",
    "- Spark will rerun any failed tasks due to failed worker nodes.\n",
    "- Spark will spill data to disk if it does not fit in memory.\n",
    "- Spark will recompute data cached on failed worker nodes.\n",
    "\n",
    "13. When is most advantageous to store DataFrame df at the MEMORY_AND_DISK storage level rather than the MEMORY_ONLY storage level?\n",
    "\n",
    "- When it is faster to read all the computed data in Dafraframe DF that cannot fit into memory from disk rather than recompute it based on its logical plan.\n",
    "\n",
    "14. Which configuration of cluster is most likely to experience a out-of-memory error in response to data skew in a single partition?\n",
    "\n",
    "- Clusters with more worker nodes.\n",
    "\n",
    "15. If I have 2 dataframes, df_a with 128GB and df_b with 1GB, it's most recommended to broadcast df_b to all worker nodes to avoid the shuffle of df_a.\n",
    "\n",
    "16. If I have a dataframe with 8 partitions and I would like to repartition it to 12 partitions I most use `df.repartition(12)`.\n",
    "\n",
    "17. The cache() operation can only cache a DataFrame at the MEMORY_AND_DISK level (the default). persist() should be used to cache a DataFrame at the MEMORY_ONLY level.\n",
    "\n",
    "18. \n",
    "\n",
    "- `spark.sql.adaptative.coalescePartitions.enabled`: \n",
    "- `spark.sql.adaptative.skewJoin.enabled`:\n",
    "- `spark.sql.autoBroadcastJoinThreshold`:\n",
    "- `spark.sql.adaptive.skeewedJoin.skewThreshold`:\n",
    "\n",
    "19. Understand the difference between types Timestamp and Date in Spark:\n",
    "\n",
    "- A Timestamp type is a date and time with a precision of milliseconds.\n",
    "\n",
    "20. Storage Levels\n",
    "\n",
    "- MEMORY_ONLY_2\n",
    "- MEMORY_AND_DISK_SER\n",
    "- MEMORY_AND_DISK\n",
    "- MEMORY_ONLY_SER_2\n",
    "- MEMORY_ONLY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |open_2023|      false|\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "def get_spark_session():\n",
    "  NESSIE_URI = \"http://nessie:19120/api/v1\"\n",
    "  MINIO_HOST = 'http://minio:9000'\n",
    "  MINIO_ACCESS_KEY = \"pTo0VEtqzA7hTKsCwCOV\"\n",
    "  MINIO_SECRET_KEY = \"B7j2Pt93vVDkYhJ9BsKBi43ZhQtMZU5iP4UznW92\"\n",
    "\n",
    "  jar_packages = [\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1\",\n",
    "    \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.95.0\",\n",
    "    \"software.amazon.awssdk:bundle:2.17.178\",\n",
    "    \"software.amazon.awssdk:url-connection-client:2.17.178\"]\n",
    "\n",
    "  spark_extensions = [\n",
    "    \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"]\n",
    "\n",
    "  conf = (\n",
    "    pyspark.SparkConf()\n",
    "      .setAppName('iceberg_hello_world')\n",
    "      .set('spark.jars.packages', ','.join(jar_packages))\n",
    "      .set('spark.sql.extensions', ','.join(spark_extensions))\n",
    "      .set('spark.sql.catalog.nessie', \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "      .set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "      .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "      .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "      .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "      .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "      .set('spark.sql.catalog.nessie.s3.endpoint', MINIO_HOST)\n",
    "      .set('spark.sql.catalog.nessie.warehouse', 's3a://warehouse')\n",
    "      .set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "      .set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    "      .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "      .set('spark.hadoop.fs.s3a.connection.timeout', '600000')\n",
    "      .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "      .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "      .set('spark.hadoop.fs.s3a.endpoint', MINIO_HOST))\n",
    "  return SparkSession.builder.config(conf=conf).master(\"spark://spark-master:7077\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "spark = get_spark_session()\n",
    "spark.sql(\"SHOW TABLES IN nessie\").show()\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# df.select(\"col_1\", \"col_2\").show()\n",
    "# df.drop(\"col_1\", \"col_2\").show()\n",
    "# df.filter(col(\"sqft\") <= 25000).show()\n",
    "# df.filter((col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30)).show()\n",
    "# df.withColumn(\"storedId\", col(\"storeId\").cast(\"string\")).show()\n",
    "# df.withColumn(\"modality\", lit(\"PHYSICAL\"))\n",
    "# df.withColumn(\"store_value_category\", col(\"store_category\".split(\"_\")[0])).withColumn(\"store_size_category\", col(\"store_category\").split(\"_\")[0])).show()\n",
    "# df.withColumn(\"productCategories\", explode(col(\"productCategories\"))).show()\n",
    "# df.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description\", \"\")).show()\n",
    "# df.distinct().show()\n",
    "# df.drop\n",
    "# df.dropDuplicates().show()\n",
    "# df.drop_duplicates().show()\n",
    "# df.dropDuplicates(subset=\"all\").show()\n",
    "# df.agg(approx_count_distinct(col(\"division\"))).alias(\"count\").show()\n",
    "# df.describe().show()\n",
    "# df.summary(*statistics).show()\n",
    "\n",
    "# df.orderBy(\"division\", ascending=False).show()\n",
    "# df.sort(\"division\", ascending=False).show()\n",
    "# df.sort(desc(\"division\")).show()\n",
    "# df.sort(col(\"division\").desc()).show()\n",
    "\n",
    "\n",
    "# df.sample(withReplacement=False, fraction=0.5, seed=42).show()\n",
    "# spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)\n",
    "# df.withColumn(\"performance\", expr(\"ASSESS_PERFORMANCE(sales, target)\")).show()\n",
    "# assessPerformance = udf(assessPerformance)\n",
    "# df.withColumn(\"performance\", assessPerformance(col(\"sales\")).show()\n",
    "# df.withColumn(\"splited_col\", split(col(\"col\"), \" \").getItem(0)).show()\n",
    "\n",
    "\n",
    "# df.sample(True, fraction=0.1, seed=42).show()\n",
    "# df.na.fill(30000, col(\"sqft\")).show()\n",
    "# df.na.fill(30000, [\"sqft\"]).show()\n",
    "# df.agg\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = spark.createDataFrame([1,2,3], IntegerType())\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
