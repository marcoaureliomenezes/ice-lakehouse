{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4731a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9fbd5004-e8b4-4f5a-8c21-c432b1837e7f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 555ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9fbd5004-e8b4-4f5a-8c21-c432b1837e7f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/11ms)\n",
      "24/09/30 00:45:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from utils import get_spark_session\n",
    "import os\n",
    "\n",
    "spark = get_spark_session(\"iceberg_DDL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78cc4f-60f0-4ff1-b49d-6382af57505c",
   "metadata": {},
   "source": [
    "# Spark DDL\n",
    "\n",
    "- To use Iceberg in Spark, first users need to configure **Spark Catalogs**.\n",
    "- Iceberg uses **Apache Spark's DataSourceV2 API** for data source and catalog implementations;\n",
    "\n",
    "#### References\n",
    "\n",
    "- https://iceberg.apache.org/docs/nightly/spark-ddl/\n",
    "\n",
    "## 1. CREATE TABLE\n",
    "\n",
    "To create an Iceberg Table use `USING ICEBERG`;\n",
    "\n",
    "```sql\n",
    "CREATE TABLE nessie.dev.sample (\n",
    "    id bigint NOT NULL COMMENT 'unique id',\n",
    "    data string,\n",
    "    category string)\n",
    "USING iceberg\n",
    "PARTITIONED BY (category);\n",
    "```\n",
    "\n",
    "### 1.1. PARTITIONED BY\n",
    "\n",
    "- PARTITIONED BY clause supports transform expressions to create **hidden partitions**.\n",
    "- \n",
    "```sql\n",
    "CREATE TABLE nessie.dev.sample (\n",
    "    id bigint NOT NULL COMMENT 'unique id',\n",
    "    data string,\n",
    "    category string)\n",
    "USING iceberg\n",
    "PARTITIONED BY (bucket(16, id), days(ts), category);\n",
    "```\n",
    "\n",
    "- Supported transformations are:\n",
    "    - `year(ts)`: partition by year;\n",
    "    - `month(ts)`: partition by month;\n",
    "    - `day(ts) or date(ts)`: equivalent to dateint partitioning;\n",
    "    - `hour(ts) or date_hour(ts)`: equivalent to dateint and hour partitioning;\n",
    "    - `bucket(N, col)`: partition by hashed value mod N buckets;\n",
    "    - `truncate(L, col)`: partition by value truncated to L;\n",
    "        - String are truncated to the given length;\n",
    "        - Integers and longs truncate to bins;\n",
    "\n",
    "### 1.2. CREATE TABLE AS SELECT\n",
    "\n",
    "- Iceberg supports CTAS as an atomic operation when using a `SparkCatalog`.\n",
    "- CTAS is supported, but is not atomic when using `SparkSessionCatalog`.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE catalog.db.sample\n",
    "USING ICEBERG\n",
    "PARTITIONED BY (partition)\n",
    "TBLPROPERTIES ('key'='value')\n",
    "AS SELECT ...\n",
    "```\n",
    "\n",
    "### 1.3. DDL CREATE Statements\n",
    "\n",
    "- With iceberg is possible to `CREATE`, `REPLACE`, `CREATE OR REPLACE` and `CREATE IF NOT EXISTS` actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8a06c9-dde1-4597-8409-9769482c212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|      dev|    ddl_test|      false|\n",
      "|      dev|partitioning|      false|\n",
      "|      dev|     table_1|      false|\n",
      "|      dev|     table_2|      false|\n",
      "|      dev|   third_ice|      false|\n",
      "+---------+------------+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- purchase_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TABLE_NAME = \"nessie.dev.ddl_test\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {TABLE_NAME} (\n",
    "    id BIGINT              NOT NULL COMMENT 'unique id',\n",
    "    name STRING            NOT NULL COMMENT 'client name',\n",
    "    age INT                NOT NULL COMMENT 'client age',\n",
    "    salary DOUBLE                   COMMENT 'last salary',\n",
    "    purchase_date STRING            COMMENT 'last purchase date')\n",
    "    \n",
    "USING ICEBERG\n",
    "PARTITIONED BY (purchase_date)\"\"\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES IN nessie\").show()\n",
    "spark.table(TABLE_NAME).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7467e-dcbf-450b-8e1c-1ee7c456d71b",
   "metadata": {},
   "source": [
    "## 2. DROP TABLE\n",
    "\n",
    "- Prior to version 0.14, **DROP TABLE only remove the table from the catalog**;\n",
    "- In order to drop table contents use DROP TABLE PURGE;\n",
    "\n",
    "```sql\n",
    "DROP TABLE catalog.db.sample;\n",
    "DROP TABLE catalog.db.sample PURGE;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7d47a-40ee-4da3-8e0c-22e92ec34673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef97e50-af54-4e12-be79-28814f664afc",
   "metadata": {},
   "source": [
    "## 3. ALTER TABLE\n",
    "\n",
    "### 3.1. ALTER TABLE without SQL Extensions\n",
    "Iceberg has full ALTER TABLE support in Spark 3, including:\n",
    "\n",
    "- Renaming table;\n",
    "- Setting or removing table properties;\n",
    "- Adding, deleting, and renaming columns;\n",
    "- Adding, deleting, and renaming nested struct fields;\n",
    "- Reordering top-level columns and nested struct fields;\n",
    "- Widening the type of int, float, and decimal fileds;\n",
    "- Marking required columns optional.\n",
    "\n",
    "#### 3.1. Renaming the table\n",
    "```sql\n",
    "ALTER TABLE catalog.db.sample RENAME TO catalog.db.new_sample;\n",
    "```\n",
    "\n",
    "#### 3.2. Changing TBLPROPERTIES\n",
    "```sql\n",
    "ALTER TABLE catalog.db.sample SET TBLPROPERTIES ('read.split.target-size'='268435456');\n",
    "ALTER TABLE catalog.db.sample UNSET TBLPROPERTIES ('read.split.target-size');\n",
    "```\n",
    "\n",
    "#### 3.3. Adding new columns\n",
    "```sql\n",
    "ALTER TABLE catalog.db.sample ADD COLUMNS (new_column string comment 'new column docs');\n",
    "ALTER TABLE catalog.db.sample ADD COLUMN new_column bigint;\n",
    "ALTER TABLE catalog.db.sample ADD COLUMN new_column bigint AFTER other_column;\n",
    "```\n",
    "\n",
    "#### 3.4. Rename existing column\n",
    "```sql\n",
    "ALTER TABLE catalog.db.sample RENAME COLUMN data TO payload;\n",
    "```\n",
    "\n",
    "#### 3.5. Drop NOT NULL constraint\n",
    "```sql\n",
    "\n",
    "ALTER TABLE catalog.db.sample ALTER COLUMN id DROP NOT NULL;\n",
    "```\n",
    "\n",
    "#### 3.6. Droping a column or nested field\n",
    "```sql\n",
    "ALTER TABLE catalog.db.sample DROP COLUMN id;\n",
    "ALTER TABLE catalog.db.sample DROP COLUMN point.z;\n",
    "\n",
    "```\n",
    "\n",
    "### 3.2. ALTER TABLE with SQL Extensions\n",
    "\n",
    "- Iceberg supports adding new partition fields to an spec using `ADD PARTITION FIELD`\n",
    "- Adding partition field is a metadata operation and does not change any of the existing table data;\n",
    "- New data will be written with the new partitioning, but existing data will remain in old partition layout;\n",
    "\n",
    "```sql\n",
    "ALTER TABLE catalog.db.sample ADD PARTITION FIELD bucket(16, id);\n",
    "ALTER TABLE catalog.db.sample ADD PARTITION FIELD truncate(4, data);\n",
    "ALTER TABLE catalog.db.sample ADD PARTITION FIELD year(ts);\n",
    "ALTER TABLE catalog.db.sample ADD PARTITION FIELD bucket(16, id) AS shard;\n",
    "\n",
    "ALTER TABLE catalog.db.sample DROP PARTITION FIELD catalog;\n",
    "ALTER TABLE catalog.db.sample DROP PARTITION FIELD bucket(16, id);\n",
    "ALTER TABLE catalog.db.sample DROP PARTITION FIELD truncate(4, data);\n",
    "ALTER TABLE catalog.db.sample DROP PARTITION FIELD year(ts);\n",
    "ALTER TABLE catalog.db.sample DROP PARTITION FIELD shard;\n",
    "\n",
    "```\n",
    "\n",
    "### 3.3. ALTER TABLE WRITE ORDERED BY\n",
    "\n",
    "Iceberg Tables can be configured with a sort order that is used to automatically sort data that is written to the table in some engines.\n",
    "\n",
    "```sql\n",
    "\n",
    "ALTER TABLE catalog.db.sample WRITE ORDERED BY category, id;\n",
    "ALTER TABLE catalog.db.sample WRITE ORDERED BY category ASC, id DESC;\n",
    "ALTER TABLE catalog.db.sample WRITE ORDERED BY category ASC NULLS LAST, id DESC NULLS FIRST;\n",
    "\n",
    "ALTER TABLE prod.db.sample WRITE LOCALLY ORDERED BY category, id\n",
    "ALTER TABLE prod.db.sample WRITE UNORDERED\n",
    "```\n",
    "\n",
    "## 4. Branching and Tagging DDL\n",
    "\n",
    "### 4.1. Branches\n",
    "\n",
    "- Branches can be created at current snapshot via the `CREATE BRANCH` statement;\n",
    "- Do not fail if the branch already exists with `IF NOT EXISTS`;\n",
    "- Update the branch if it already exists with `CREATE OR REPLACE`;\n",
    "- Create a branch at a specific snapshot;\n",
    "- Create a branch with a specified retention period.\n",
    "\n",
    "```sql\n",
    "\n",
    "ALTER TABLE prod.db.sample CREATE BRANCH `audit-branch`\n",
    "ALTER TABLE prod.db.sample CREATE BRANCH IF NOT EXISTS `audit-branch`\n",
    "ALTER TABLE prod.db.sample CREATE OR REPLACE BRANCH `audit-branch`\n",
    "ALTER TABLE prod.db.sample CREATE BRANCH `audit-branch` AS OF VERSION 1234\n",
    "    \n",
    "-- CREATE audit-branch at snapshot 1234, retain audit-branch for 30 days, and retain the latest 30 days. The latest 3 snapshot snapshots, and 2 days worth of snapshots.\n",
    "ALTER TABLE prod.db.sample CREATE BRANCH `audit-branch` AS OF VERSION 1234 RETAIN 30 DAYS WITH SNAPSHOT RETENTION 3 SNAPSHOTS 2 DAYS\n",
    "```\n",
    "\n",
    "### 4.2. Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ecae3e-70ab-4294-aee6-33cee872313c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d80b6-8000-4d1a-9725-08e4ce8015f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd283b0-d43f-4de2-9066-8d4f0a92a3e8",
   "metadata": {},
   "source": [
    "# Iceberg TBLPROPERTIES\n",
    "\n",
    "## File Formats and Compression Types \n",
    "\n",
    "## Column Metrics Tracking\n",
    "\n",
    "- If a table has many columns, tracking the metrics for all columns can get very expensive to your writes;\n",
    "- You can manage metrics and turn them off for columns where they may not be relevant to your query patterns;\n",
    "\n",
    "## Parquet Vectorization\n",
    "\n",
    "- By default parquet vectorization will be turned off.\n",
    "- This change the behavior of how data is delivered from one row at a time to column batches which can improve performance.\n",
    "- This should generally be turned on; It defaults to being turned off;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981b0f7-8e5f-4276-b62b-f0c905fd9b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c229982e-f274-4865-9c1f-89fa28f78cec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a33caa-fd0b-47e9-b565-221cf69ffdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0ca9951-cb50-450a-81b5-45b389a3e1bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a21f0-ab99-4644-ac8e-3e7cce2ba616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e1873-1ae5-4a30-8d27-51023999c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nessie.dev.table_2 (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  age INT,\n",
    "  salary FLOAT,\n",
    "  last_purchase_date DATE\n",
    ") USING iceberg TBLPROPERTIES (\n",
    "  'write.delete.mode'='copy-on-write',\n",
    "  'write.update.mode'='copy-on-write',\n",
    "  'write.merge.mode'='copy-on-write',\n",
    "  'write.format.default'='parquet',\n",
    "  'write.delete.format.default'='avro',\n",
    "  'write.parquet.compression-codec'='zstd',\n",
    "  'write.metadata.delete-after-commit.enabled'='true',\n",
    "  'write.metadata.previous-versions-max=50,\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    ") PARTITIONED BY ( last_purchase_date ) \n",
    "\"\"\")\n",
    "spark.sql(\"DESCRIBE EXTENDED nessie.dev.table_2\").show(100)\n",
    "spark.sql(\"SHOW TBLPROPERTIES nessie.dev.table_2\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f91383-a3cb-4a3f-b0d1-5ee84507661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nessie.dev.table_2 (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  age INT,\n",
    "  salary FLOAT,\n",
    "  last_purchase_date DATE\n",
    ") USING iceberg TBLPROPERTIES (\n",
    "  'write.delete.mode'='copy-on-write',\n",
    "  'write.update.mode'='copy-on-write',\n",
    "  'write.merge.mode'='copy-on-write',\n",
    "  'write.format.default'='parquet',\n",
    "  'write.delete.format.default'='avro',\n",
    "  'write.parquet.compression-codec'='zstd',\n",
    "  'write.metadata.delete-after-commit.enabled'='true',\n",
    "  'write.metadata.previous-versions-max=50,\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    ") PARTITIONED BY ( last_purchase_date ) \n",
    "\"\"\")\n",
    "spark.sql(\"DESCRIBE EXTENDED nessie.dev.table_2\").show(100)\n",
    "spark.sql(\"SHOW TBLPROPERTIES nessie.dev.table_2\").show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
