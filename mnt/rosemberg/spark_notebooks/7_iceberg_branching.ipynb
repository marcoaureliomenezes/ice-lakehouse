{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4731a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9c2759e7-67bd-4006-9621-30205071ac19;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 513ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9c2759e7-67bd-4006-9621-30205071ac19\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/8ms)\n",
      "24/09/29 22:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- last_purchase: timestamp (nullable = true)\n",
      " |-- last_purchase_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from utils import get_spark_session\n",
    "import time\n",
    "from data_generator import gen_bulk_data\n",
    "\n",
    "spark = get_spark_session(\"iceberg_nessie_branching\")\n",
    "df_test = gen_bulk_data(spark, 10**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c0d6e-1227-4680-ad30-b084e8ca5599",
   "metadata": {},
   "source": [
    "### Branching steps\n",
    "\n",
    "1. Create a Spark Dataframe;\n",
    "2. Load the data into a TEMPORARY VIEW;\n",
    "3. Create a table using CTAS statement and the view created above;\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce833d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-------+-------------------+------------------+\n",
      "|     id|                name|age| salary|      last_purchase|last_purchase_date|\n",
      "+-------+--------------------+---+-------+-------------------+------------------+\n",
      "|3343488|       Maysa da Cruz| 58|1004.11|1972-08-08 00:19:06|        1972-08-08|\n",
      "|7937829|       Sofia da Rosa| 37| 996.98|1971-05-13 02:51:54|        1971-05-13|\n",
      "|7253113|Carlos Eduardo Ramos| 55|1005.93|1976-08-07 03:14:02|        1976-08-07|\n",
      "|7993723| Davi Miguel Costela| 44|1003.76|1973-02-09 14:45:42|        1973-02-09|\n",
      "|6938021|        Ayla Pimenta| 16| 993.98|1978-06-29 08:08:44|        1978-06-29|\n",
      "|4513143|Srta. Evelyn Nasc...| 78| 997.23|1990-11-09 14:44:47|        1990-11-09|\n",
      "| 721575|      Miguel Sampaio| 57| 991.53|1995-01-29 07:23:03|        1995-01-29|\n",
      "|5126821| José Pedro Monteiro| 61|  993.2|1995-06-11 12:22:44|        1995-06-11|\n",
      "|7965086|    Sabrina Nogueira| 28|1008.09|1988-10-07 22:43:38|        1988-10-07|\n",
      "|8496275|       Isabel Farias| 61| 996.34|1996-06-28 11:15:51|        1996-06-28|\n",
      "|6926467|Srta. Ana Sophia ...| 52| 994.64|1987-07-24 20:46:23|        1987-07-24|\n",
      "|1047437|    Valentim Camargo| 88|1005.52|1989-08-12 08:10:51|        1989-08-12|\n",
      "|5471064|Dr. Anthony Gabri...|  3| 995.66|1983-12-21 05:33:46|        1983-12-21|\n",
      "|6457140|         Ravi Mendes| 43| 988.29|2006-01-22 21:29:48|        2006-01-22|\n",
      "| 256190|Dra. Carolina Tei...| 82|1008.49|2011-10-09 17:08:32|        2011-10-09|\n",
      "|5292513|     Dr. Lucca Silva|  8|1004.29|1971-06-05 00:30:25|        1971-06-05|\n",
      "|1368765|Dra. Marina Gonça...| 75|1003.19|2003-07-02 18:54:36|        2003-07-02|\n",
      "|7225161|        Bianca Pires| 94|1005.61|1985-06-24 11:05:44|        1985-06-24|\n",
      "|7286919|       Paulo Freitas| 81| 992.93|1987-04-03 23:37:19|        1987-04-03|\n",
      "|7051393|   Ana Lívia Machado|  9|1000.21|1992-06-25 03:36:17|        1992-06-25|\n",
      "+-------+--------------------+---+-------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch|main|ba78e1776528d4abe...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".                \n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Num lines after creation: 10000\n",
      "+-------+-------------+--------------------+\n",
      "|refType|         name|                hash|\n",
      "+-------+-------------+--------------------+\n",
      "| Branch|feature_other|85aaa5c12490b657d...|\n",
      "+-------+-------------+--------------------+\n",
      "\n",
      "+-------+-------------+--------------------+\n",
      "|refType|         name|                hash|\n",
      "+-------+-------------+--------------------+\n",
      "| Branch|feature_other|85aaa5c12490b657d...|\n",
      "+-------+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num lines after deletion in feature branch: 5093\n",
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch|main|85aaa5c12490b657d...|\n",
      "+-------+----+--------------------+\n",
      "\n",
      "Num lines after deletion in main branch: 10000\n",
      "Num lines after merge of feature_other into main branch: 5093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TABLE_NAME = \"nessie.dev.branching\"\n",
    "df_test.createOrReplaceTempView(\"df_test\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM df_test\").show()\n",
    "\n",
    "spark.sql(\"USE REFERENCE main IN nessie\").show()\n",
    "\n",
    "spark.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} USING iceberg AS SELECT * FROM df_test\").show()\n",
    "print(f\"Num lines after creation: {spark.table('nessie.dev.branching').count()}\")\n",
    "\n",
    "spark.sql(\"CREATE BRANCH IF NOT EXISTS feature_other IN nessie\").show()\n",
    "spark.sql(\"USE REFERENCE feature_other IN nessie\").show()\n",
    "\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE age > 50\")\n",
    "print(f\"Num lines after deletion in feature branch: {spark.table(TABLE_NAME).count()}\")\n",
    "\n",
    "spark.sql(\"USE REFERENCE main IN nessie\").show()\n",
    "print(f\"Num lines after deletion in main branch: {spark.table(TABLE_NAME).count()}\")\n",
    "\n",
    "spark.sql(\"MERGE BRANCH feature_other INTO main IN nessie\")\n",
    "time.sleep(30)\n",
    "\n",
    "print(f\"Num lines after merge of feature_other into main branch: {spark.table(TABLE_NAME).count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5848a0-0ad9-4d72-a1d8-07c94ab7d99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num lines after merge of feature_other into main branch: 5093\n",
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|    OK|\n",
      "+------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num lines after merge of feature_other into main branch: {spark.table(TABLE_NAME).count()}\")\n",
    "\n",
    "spark.sql(\"DROP BRANCH IF EXISTS feature_other IN nessie\").show()\n",
    "spark.sql(\"DROP TABLE nessie.dev.branching\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef256f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
