{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc3d720-9043-46da-af4c-a94152c5843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2f60b165-2794-4a64-ba8d-d780dcf93d1f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 457ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2f60b165-2794-4a64-ba8d-d780dcf93d1f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/10ms)\n",
      "24/09/29 22:52:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- last_purchase: timestamp (nullable = true)\n",
      " |-- last_purchase_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from utils import get_spark_session\n",
    "import time\n",
    "from data_generator import gen_bulk_data\n",
    "\n",
    "spark = get_spark_session(\"iceberg_table_partitioning\")\n",
    "df_test = gen_bulk_data(spark, 10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef256f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-------+-------------------+------------------+\n",
      "|     id|                name|age| salary|      last_purchase|last_purchase_date|\n",
      "+-------+--------------------+---+-------+-------------------+------------------+\n",
      "|9928165|          Gabriel Sá|  0|1011.11|1983-06-03 14:02:36|        1983-06-03|\n",
      "|9638342|      Manuella Porto|  0| 981.63|2013-01-09 03:03:25|        2013-01-09|\n",
      "| 294849|       Bryan Pereira|  0|1000.09|1999-03-13 21:29:49|        1999-03-13|\n",
      "|3231746|       Evelyn Novaes|  0| 989.01|1981-09-30 06:29:19|        1981-09-30|\n",
      "|5698933|    Benjamim Camargo|  0| 989.02|1989-08-13 04:48:17|        1989-08-13|\n",
      "|9952876|  Dra. Sabrina Jesus|  0|1000.37|1984-11-05 09:35:27|        1984-11-05|\n",
      "|9910922|  Sra. Isabela Rocha|  0| 998.74|2018-03-05 14:37:04|        2018-03-05|\n",
      "|5217730|      Olivia Fonseca|  0|1011.34|1971-07-08 05:18:33|        1971-07-08|\n",
      "|4456269|     Ayla Nascimento|  0|1001.05|2018-01-09 22:22:08|        2018-01-09|\n",
      "|1561052|Srta. Melissa da ...|  0| 986.83|2020-12-11 22:13:00|        2020-12-11|\n",
      "|8996948|        Bruno Duarte|  0| 998.97|1988-07-27 03:43:24|        1988-07-27|\n",
      "| 329915| Dra. Luna Rodrigues|  0|  995.0|1974-12-26 06:29:07|        1974-12-26|\n",
      "|1947572| Maria Helena Mendes|  0|1011.53|1974-04-06 23:49:04|        1974-04-06|\n",
      "|5074931|         Bruna Porto|  0| 989.13|2018-01-25 19:01:10|        2018-01-25|\n",
      "|6834072|    Sra. Isis Borges|  0| 999.97|2004-11-15 02:33:08|        2004-11-15|\n",
      "|1967888|       Isaque Farias|  0| 996.97|1985-05-09 04:21:25|        1985-05-09|\n",
      "|5859014|    João Vitor Lopes|  0| 988.02|1994-11-14 03:05:23|        1994-11-14|\n",
      "|2564749|    Isabella Peixoto|  0|1021.36|1975-05-23 12:07:52|        1975-05-23|\n",
      "|4664105|     Antônio Barbosa|  0| 998.16|2005-03-01 06:31:28|        2005-03-01|\n",
      "|9161331|      Luiza Siqueira|  0| 1001.1|1986-10-07 19:24:43|        1986-10-07|\n",
      "+-------+--------------------+---+-------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TABLE_NAME = \"nessie.dev.partitioning\"\n",
    "\n",
    "df_test.createOrReplaceTempView(\"df_test\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {TABLE_NAME} USING ICEBERG PARTITIONED BY (age) AS SELECT * FROM df_test ORDER BY age\n",
    "\"\"\")\n",
    "\n",
    "spark.table(TABLE_NAME).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d8f201-3152-45af-a42b-a4e46d785c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                       101|                  1976|               492765|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"ALTER TABLE {TABLE_NAME} ADD PARTITION FIELD truncate(1, name)\").show()\n",
    "\n",
    "spark.sql(f\"CALL nessie.system.rewrite_data_files('{TABLE_NAME}')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973c5e6b-c71f-4c5e-85bf-838a539ed84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-09-29 22:52:...|8248509610644572393|               NULL|              false|\n",
      "|2024-09-29 22:55:...|5582960184519021905|               NULL|               true|\n",
      "|2024-09-29 23:02:...|8285212853883752419|5582960184519021905|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:44:39 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/09/30 00:44:39 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "24/09/30 00:44:39 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@72556420[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@7a7ce8ec[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@6d2e8c73]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@321f6aae[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {TABLE_NAME}.history;\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20b60a-efb4-457a-8cd0-47766a0f053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
