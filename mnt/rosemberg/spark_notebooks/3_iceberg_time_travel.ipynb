{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc3d720-9043-46da-af4c-a94152c5843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-21b63894-4803-45fd-9c1a-b16d0dac49bc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 853ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.95.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-21b63894-4803-45fd-9c1a-b16d0dac49bc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/16ms)\n",
      "24/09/29 21:24:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- last_purchase: timestamp (nullable = true)\n",
      " |-- last_purchase_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from utils import get_spark_session\n",
    "import time\n",
    "from data_generator import gen_bulk_data\n",
    "\n",
    "spark = get_spark_session(\"iceberg_table_partitioning\")\n",
    "df_test = gen_bulk_data(spark, 10**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d180520-cf44-4357-a117-e878492933e1",
   "metadata": {},
   "source": [
    "# Time Travel\n",
    "\n",
    "- **What is it?** The ability to query the table as it was during a previous valid snapshot;\n",
    "- **Why use it?**:\n",
    "    - To run ML model tests on the same data it was tested previously;\n",
    "    - Avoid the need for snapshotting the table with CTAS statements;\n",
    "    - For quality control to assess data before and after updates;\n",
    "      \n",
    "- **Limitations**: Can only time-travel to valid snapshots (not expired ones);\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e5588-afdd-4c4f-8078-f1ca726b8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"as-of-timestamp\", 499162860000).format(\"iceberg\").load(\"path/to/table\")\n",
    "spark.read.option(\"snapshot-id\", 10963874102873L).format(\"iceberg\").load(\"path/to/table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a45a8-a9aa-449f-a57d-d551391ec72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20b60a-efb4-457a-8cd0-47766a0f053d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3b42ee2-4bee-4420-aad6-e55837d8392c",
   "metadata": {},
   "source": [
    "# Iceberg Tables Maintainance\n",
    "\n",
    "## 1. Expiring Snapshots\n",
    "\n",
    "```sql\n",
    "CALL prod.system.expire_snapshots(\n",
    "    table => 'db.sample',\n",
    "    older_than => now(),\n",
    "    retain_last => 30\n",
    ");\n",
    "CALL prod.system.expire_snapshots('db.sample', TIMESTAMP '2024-09-01 00:00:00.000', 100);\n",
    "```\n",
    "\n",
    "## 2. Rewriting Data Files and Manifests (Compaction)\n",
    "\n",
    "- To avoid the small file problem, clean up deleted records or optimize the data further with sorting we can re-write data files;\n",
    "- In the example below we rewrite the data files using a \"sort\" strategy which sorts the records along with rewriting item.\n",
    "- The above strategy will take longer than the \"binpack\" strategy which mainly rewrites data without sorting.\n",
    "\n",
    "### Rewriting data files and Manifests\n",
    "\n",
    "```sql\n",
    "CALL catalog.system.rewrite_data_files(\n",
    "    table => 'db.sample',\n",
    "    strategy => 'sort',\n",
    "    sort_order => 'id DESC NULL LAST, name ASC NULLS FIRST'\n",
    "```\n",
    "### Rewriting Manifests only\n",
    "\n",
    "```sql\n",
    "CALL catalog.system.rewrite_manifests('db_sample')\n",
    "```\n",
    "\n",
    "## 3. Deleting Orphan Files\n",
    "\n",
    "- Running **remove_orphan_files** procedure will allow you to clean up any files that may have not been deleted while expiring snapshots;\n",
    "- These files not associated with your table are called orphan files;\n",
    "- By default will look in the tables data directory, but in case there are data files in other directories (because of migrate or add_files) you can also specify a directory to clean;\n",
    "\n",
    "\n",
    "```sql\n",
    "CALL catalog.system.remove_orphan_files(table => 'db.sample')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa210342-2480-48cd-a625-1a4f68e709de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183033f-515d-4094-8250-b156ababec4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
