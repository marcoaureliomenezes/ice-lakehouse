{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc3d720-9043-46da-af4c-a94152c5843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- last_purchase: timestamp (nullable = true)\n",
      " |-- last_purchase_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from utils import get_spark_session\n",
    "import os\n",
    "\n",
    "spark = get_spark_session(\"iceberg_transactions_PySpark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef256f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de linhas após criação: 100\n",
      "Numero de linhas após append: 200\n"
     ]
    }
   ],
   "source": [
    "TABLE_NAME = \"nessie.dev.table_1\"\n",
    "\n",
    "df_test.writeTo(TABLE_NAME).createOrReplace()\n",
    "print(f\"Numero de linhas após criação: {spark.table(TABLE_NAME).count()}\")\n",
    "df_test.writeTo(TABLE_NAME).append()\n",
    "print(f\"Numero de linhas após append: {spark.table(TABLE_NAME).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973c5e6b-c71f-4c5e-85bf-838a539ed84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-09-29 15:37:...|5565358984773854541|               NULL|              false|\n",
      "|2024-09-29 15:37:...|5484199323931666385|5565358984773854541|              false|\n",
      "|2024-09-29 15:38:...| 263071444238144259|               NULL|              false|\n",
      "|2024-09-29 15:38:...|2128714391020491279| 263071444238144259|              false|\n",
      "|2024-09-29 16:49:...| 463466068859629968|               NULL|              false|\n",
      "|2024-09-29 16:49:...| 550167812940096497| 463466068859629968|              false|\n",
      "|2024-09-29 17:10:...| 718049196851715350|               NULL|              false|\n",
      "|2024-09-29 17:11:...|5445461876262848214| 718049196851715350|              false|\n",
      "|2024-09-29 17:26:...|3086337887324965106|               NULL|               true|\n",
      "|2024-09-29 17:26:...|2796627309617165593|3086337887324965106|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM nessie.dev.table_1.history;\n",
    "\"\"\").show()\n",
    "\n",
    "# spark.table(\"nessie.dev.table_1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0baa2dd-a04e-4c5b-82b4-6d7e85c93878",
   "metadata": {},
   "source": [
    "## Merge on Read and Copy-on-write\n",
    "\n",
    "### When to use?\n",
    "\n",
    "- **Copy-on-write**: Daily batch jobs where write speed is less a priority and read time is a high priority;\n",
    "- **Merge-on-read (Position Deletes)**: Streaming and higher frequency batch (hourly) where write speed is very important with minor cost to read times. Regular compaction should be scheduled;\n",
    "- **Merge-on-read (Equality Deletes)**: Very write intensive jobs where position deletes still aren't fast enough, much larger cost to reading so frequent compaction jobs will be necessary to manage;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27f039c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nessie.dev.table_2 (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  age INT,\n",
    "  salary FLOAT,\n",
    "  last_purchase_date DATE\n",
    ") USING iceberg TBLPROPERTIES (\n",
    "  'write.delete.mode'='copy-on-write',\n",
    "  'write.update.mode'='copy-on-write',\n",
    "  'write.merge.mode'='copy-on-write'\n",
    ") PARTITIONED BY ( last_purchase_date ) \n",
    "\"\"\")\n",
    "spark.sql(\"DESCRIBE EXTENDED nessie.dev.table_2\").show(100)\n",
    "spark.sql(\"SHOW TBLPROPERTIES nessie.dev.table_2\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc2da410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                  id|                 int|   NULL|\n",
      "|                name|              string|   NULL|\n",
      "|                 age|                 int|   NULL|\n",
      "|              salary|               float|   NULL|\n",
      "|  last_purchase_date|                date|   NULL|\n",
      "|# Partition Infor...|                    |       |\n",
      "|          # col_name|           data_type|comment|\n",
      "|  last_purchase_date|                date|   NULL|\n",
      "|                    |                    |       |\n",
      "|  # Metadata Columns|                    |       |\n",
      "|            _spec_id|                 int|       |\n",
      "|          _partition|struct<last_purch...|       |\n",
      "|               _file|              string|       |\n",
      "|                _pos|              bigint|       |\n",
      "|            _deleted|             boolean|       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|  nessie.dev.table_2|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Location|s3a://warehouse/d...|       |\n",
      "|            Provider|             iceberg|       |\n",
      "|               Owner|               spark|       |\n",
      "|    Table Properties|[current-snapshot...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "| current-snapshot-id|                none|\n",
      "|              format|     iceberg/parquet|\n",
      "|      format-version|                   2|\n",
      "|          gc.enabled|               false|\n",
      "|    nessie.commit.id|229c5d85b30fb62ec...|\n",
      "|   write.delete.mode|       copy-on-write|\n",
      "|    write.merge.mode|       copy-on-write|\n",
      "|write.metadata.de...|               false|\n",
      "|write.parquet.com...|                zstd|\n",
      "|   write.update.mode|       copy-on-write|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ebbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c20b60a-efb4-457a-8cd0-47766a0f053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484ae2b-e01d-4032-a599-aa238ab64286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
